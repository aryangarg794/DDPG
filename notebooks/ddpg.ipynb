{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of DDPG \n",
    "\n",
    "Reference Paper\n",
    "\n",
    "[1] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous Control with Deep Reinforcement Learning. International Conference on Learning Representations (ICLR) 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "\n",
    "from copy import deepcopy\n",
    "from utils.replay import ReplayBuffer\n",
    "from utils.metrics import RollingAverage\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = np.array(mu)\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt \\\n",
    "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        hidden_layers: list = list([400, 300]), \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Critic, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        # the layers before we add the action\n",
    "        self.before_action = nn.Sequential(\n",
    "            nn.Linear(self.env.observation_space.shape[0], hidden_layers[0]), # note: this assumes obs spaces of shape (N,) ie 1d\n",
    "            nn.BatchNorm1d(hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_layers[0], hidden_layers[1]), \n",
    "            nn.BatchNorm1d(hidden_layers[1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # add the action input\n",
    "        self.add_action = nn.Sequential(\n",
    "            nn.Linear(hidden_layers[1] + self.env.action_space.shape[0], 1),         \n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        obs: torch.Tensor,\n",
    "        action: torch.Tensor\n",
    "    ):\n",
    "        representations = self.before_action(obs)\n",
    "        rep_and_act = torch.hstack((representations, action))\n",
    "        q_val = self.add_action(rep_and_act)\n",
    "        return q_val\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        action_max: float | int = 1,\n",
    "        hidden_layers: list = list([400, 300]),\n",
    "        theta: float = 0.15, \n",
    "        sigma: float = 0.2,\n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Actor, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.env = env\n",
    "        self.action_max = action_max\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.env.observation_space.shape[0], hidden_layers[0]),\n",
    "            nn.BatchNorm1d(hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            nn.BatchNorm1d(hidden_layers[1]),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_layers[1], 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.ounoise = OrnsteinUhlenbeckActionNoise(mu=[0], theta=theta, sigma=sigma)\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        obs: torch.Tensor,\n",
    "    ): \n",
    "        return self.layers(obs) * self.action_max\n",
    "    \n",
    "    def sample_actions(\n",
    "        self, \n",
    "        obs: torch.Tensor,\n",
    "        noise: float\n",
    "    ): \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self(obs)\n",
    "        self.train()\n",
    "        return actions.cpu().detach().numpy() + self.ounoise.noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env: gym.Env,\n",
    "    action_max: int = 1, \n",
    "    batch_size: int = 32, \n",
    "    timesteps: int = 1000000,\n",
    "    capacity: int = 100000, \n",
    "    preload: int = 10000, \n",
    "    gamma: float = 0.99, \n",
    "    tau: float = 0.001, \n",
    "    lr_actor: float = 0.0001,\n",
    "    lr_critic: float = 0.001, \n",
    "    expl_noise: float = 1, \n",
    "    window: int = 10, \n",
    "    wd: float = 0.01,\n",
    "    val_freq: int = 10000, \n",
    "    num_val_eps: int = 2, \n",
    "    device: str = 'cpu',\n",
    "    save_step: int = 850000\n",
    "):\n",
    "    # setup\n",
    "    metrics = RollingAverage(window_size=window)\n",
    "    buffer = ReplayBuffer(buffer_len=capacity)\n",
    "    \n",
    "    actor = Actor(env, action_max=action_max).to(device)\n",
    "    critic = Critic(env).to(device)\n",
    "    actor_target = Actor(env, action_max=action_max).to(device)\n",
    "    critic_target = Critic(env).to(device)\n",
    "    \n",
    "    actor_target.load_state_dict(actor.state_dict())\n",
    "    critic_target.load_state_dict(critic.state_dict())\n",
    "    \n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "    critic_optimizer = torch.optim.AdamW(critic.parameters(), lr=lr_critic, weight_decay=wd)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    val_env = deepcopy(env)\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # preload the env\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    for _ in tqdm(range(preload)):\n",
    "        action = env.action_space.sample()\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        buffer.update(obs.squeeze(), action.squeeze(), reward, obs_prime.squeeze(), done)\n",
    "        \n",
    "        obs = obs_prime\n",
    "        if done: \n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    # actual training loop \n",
    "    for step in range(1, timesteps):\n",
    "        start_time = time.time()\n",
    "        if not actor.training:\n",
    "            actor.train()\n",
    "        \n",
    "        sampled_action = actor.sample_actions(torch.as_tensor(obs, dtype=torch.float32, device=device).view(1, -1), \n",
    "                                              noise=expl_noise)\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(sampled_action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        buffer.update(obs.squeeze(), sampled_action.squeeze(), reward, obs_prime.squeeze(), done)\n",
    "        \n",
    "        obs = obs_prime\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            actor.ounoise.reset()\n",
    "        \n",
    "        batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = buffer.sample(\n",
    "            batch_size, device=device)\n",
    "        \n",
    "        batch_rewards = batch_rewards.view(-1, 1)\n",
    "        batch_dones = batch_dones.view(-1, 1) \n",
    "        batch_actions = batch_actions.view(-1, 1) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_actions = actor_target(batch_next_obs)\n",
    "            q_targets = critic_target(batch_next_obs, target_actions)\n",
    "            \n",
    "        td_target = torch.where(batch_dones, batch_rewards, batch_rewards + gamma * q_targets)  \n",
    "        \n",
    "        q_values = critic(batch_obs, batch_actions) \n",
    "        loss_critic = mse_loss(q_values, td_target)\n",
    "        \n",
    "        \n",
    "        critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        actor_actions = actor(batch_obs)\n",
    "        actor_values = critic(batch_obs, actor_actions)\n",
    "        loss_actor = -(actor_values).mean()\n",
    "        \n",
    "        actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        # soft target update \n",
    "        for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param + (1-tau) * target_param)\n",
    "            \n",
    "        for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param + (1-tau) * target_param) \n",
    "             \n",
    "        # val loop \n",
    "        if step % val_freq == 0:\n",
    "            actor.eval()\n",
    "            for _ in range(num_val_eps):\n",
    "                obs_val, _ = val_env.reset()\n",
    "                done_val = False\n",
    "                ep_reward = 0 \n",
    "                while not done_val:\n",
    "                    with torch.no_grad():\n",
    "                        action = actor(torch.as_tensor(obs_val, dtype=torch.float32,\n",
    "                                                       device=device).view(1, -1)).cpu().detach().numpy() \n",
    "                        \n",
    "                    obs_prime_val, reward_val, terminated, truncated, _ = val_env.step(action)\n",
    "                    ep_reward += reward_val\n",
    "                    \n",
    "                    obs_val = obs_prime_val\n",
    "                    done_val = terminated or truncated\n",
    "                    \n",
    "                metrics.update(ep_reward)                            \n",
    "                \n",
    "            if step > save_step and metrics.get_average < best_reward:\n",
    "                torch.save({\n",
    "                    'actor_state_dict' : actor.state_dict(),\n",
    "                    'critic_state_dict' : critic.state_dict()\n",
    "                })\n",
    "        \n",
    "        time_per_step = time.time() - start_time\n",
    "        print(f'Step {step} | Average Val Reward: {metrics.get_average} | Time per step {time_per_step:.5f}', end='\\r')\n",
    "    \n",
    "    return actor, critic, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 5973/10000 [00:00<00:00, 29888.68it/s]"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "actor, critic, metrics = train(env, device=device, val_freq=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
